{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dbc6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install qdrant-client openai dotenv langchain-openai pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20490866",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "\n",
    "# ==== Configuration ====\n",
    "#TODO: replace with your actual keys\n",
    "QDRANT_URL = \"your_qdrant_url\"       # e.g. \"https://xxxx-xxxxx.eu-central.aws.cloud.qdrant.io\"\n",
    "QDRANT_API_KEY = \"your_qdrant_api_key\"     # from your Qdrant Cloud dashboard\n",
    "OPENAI_API_KEY = \"your_openai_api_key\"\n",
    "EMBED_MODEL = \"text-embedding-3-small\"  # Must match the one used when uploading\n",
    "# ========================\n",
    "\n",
    "# === Initialize clients ===\n",
    "client = OpenAI()\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model_name=\"gpt-4.1-mini\",\n",
    ")\n",
    "\n",
    "qdrant = QdrantClient(\n",
    "    url=QDRANT_URL,\n",
    "    api_key=QDRANT_API_KEY,\n",
    ")\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "#TODO: add more channels if needed\n",
    "collection_to_language = {\n",
    "    \"bluepigeon0810_videos\": \"Traditional Chinese\",\n",
    "}\n",
    "\n",
    "\n",
    "def translate_query_if_needed(query: str, target_lang=\"English\") -> str:\n",
    "    \"\"\"\n",
    "    Use an LLM to detect the query language and translate to English only if needed.\n",
    "    Keeps cost low by combining detection + translation in one step.\n",
    "    \"\"\"\n",
    "    detection_prompt = f\"\"\"\n",
    "    Detect the language of the following text. \n",
    "    If it's already in {target_lang}, reply with \"{target_lang}\".\n",
    "    Otherwise, translate it into {target_lang} and output only the translation.\n",
    "\n",
    "    Text: {query}\n",
    "    \"\"\"\n",
    "\n",
    "    detection = client.chat.completions.create(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a precise language detector and translator.\"},\n",
    "            {\"role\": \"user\", \"content\": detection_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    result = detection.choices[0].message.content.strip()\n",
    "\n",
    "    if result.upper() == target_lang.upper():\n",
    "        print(f\"üåê Query is already in {target_lang} ‚Äî skipping translation.\")\n",
    "        return query\n",
    "    else:\n",
    "        print(f\"üåê Translated query: {result}\")\n",
    "        return result\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "# === Define search function ===\n",
    "def search_videos(collection_name: str, query: str, top_k: int = 3):\n",
    "    \"\"\"Embed the query and search the Qdrant vector store.\"\"\"\n",
    "    # 1Ô∏è‚É£ Get embedding for the query\n",
    "    query = translate_query_if_needed(query, target_lang=collection_to_language.get(collection_name, \"English\"))\n",
    "    emb = client.embeddings.create(\n",
    "        model=EMBED_MODEL,\n",
    "        input=query\n",
    "    ).data[0].embedding\n",
    "\n",
    "    # 2Ô∏è‚É£ Perform search in Qdrant\n",
    "    response = qdrant.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=emb,\n",
    "        limit=top_k,\n",
    "        with_payload=True,  \n",
    "        with_vectors=False   \n",
    "    )\n",
    "    return response.points\n",
    "    # return results\n",
    "\n",
    "\n",
    "# === Define structured response schema ===\n",
    "class QueryResponse(BaseModel):\n",
    "    can_be_answered: bool = Field(\n",
    "        ..., description=\"Whether the question can be answered based on the video transcript.\"\n",
    "    )\n",
    "    summary: str = Field(\n",
    "        ..., description=\"A concise summary answer if it can be answered, otherwise an empty string.\"\n",
    "    )\n",
    "    start_time: str = Field(\n",
    "        ..., description=\"The start time (HH:MM:SS) where the answer appears, or empty string if not applicable.\"\n",
    "    )\n",
    "    end_time: str = Field(\n",
    "        ..., description=\"The end time (HH:MM:SS) where the answer appears, or empty string if not applicable.\"\n",
    "    )\n",
    "\n",
    "class FinalSummary(BaseModel):\n",
    "    can_be_answered: bool = Field(\n",
    "        ..., description=\"Whether the question can be answered based on the combined video analyses.\"\n",
    "    )\n",
    "    final_answer: str = Field(\n",
    "        ..., description=\"The final answer derived from the video analyses.\"\n",
    "    )\n",
    "    sources: list[str] = Field(\n",
    "        ..., description=\"List of source video IDs that contributed to the final answer.\"\n",
    "    )\n",
    "    time_ranges: list[str] = Field(\n",
    "        ..., description=\"List of source time ranges (HH:MM:SS - HH:MM:SS) corresponding to each source video.\"\n",
    "    )\n",
    "\n",
    "# === Define system prompt for the agent ===\n",
    "\n",
    "SYSTEM_PROMPT = [\n",
    "    \"You are an intelligent video transcript analyst.\",\n",
    "    \"Given:\",\n",
    "    \"- A video transcript\",\n",
    "    \"- The upload date of the video\",\n",
    "    \"- A user question\",\n",
    "    \"\",\n",
    "    \"Your job is to:\",\n",
    "    \"1. Determine if the question can be answered from the transcript.\",\n",
    "    \"2. If yes:\",\n",
    "    \"   - Provide a clear and detailed summary answer.\",\n",
    "    \"   - Estimate the start and end times (HH:MM:SS format) where the relevant content appears, based on transcript clues.\",\n",
    "    \"3. If not, respond with can_be_answered = false and leave other fields empty.\",\n",
    "    \"\",\n",
    "    \"Temporal reasoning rules:\",\n",
    "    \"- Always consider both the **upload date** and the **time period described** within the transcript.\",\n",
    "    # \"- If the transcript clearly refers to a specific time period (e.g., ‚Äúin 2018‚Äù, ‚Äúduring the pandemic‚Äù, ‚Äúearlier that year‚Äù), treat the statements as referring to that period ‚Äî not necessarily the upload date.\",\n",
    "    # \"- If the user‚Äôs question asks about a specific year or period, only include information that explicitly refers to or accurately describes that same time frame.\",  \n",
    "    \"- Be objective and factual ‚Äî do not guess or infer information beyond what the transcript supports.\",\n",
    "    \"- The output language **must match the user question‚Äôs language** exactly.\",\n",
    "]\n",
    "SYSTEM_PROMPT = \"\\n\".join(SYSTEM_PROMPT)\n",
    "\n",
    "\n",
    "JUDGE_PROMPT = [\n",
    "    \"You are an expert fact-checker and video analyst.\",\n",
    "    \"\",\n",
    "    \"**Important:** You may ONLY use the information explicitly provided in or inferred from the given video transcript analyses, summaries, time ranges, and upload dates. \",\n",
    "    \"Do NOT use any external knowledge, assumptions, or prior information about the topic.\",\n",
    "    \"\",\n",
    "    \"Given multiple video transcript analyses (with summaries, time ranges, and upload dates), produce a **concise, user-facing answer**:\",\n",
    "    \"\",\n",
    "    \"1. **Conciseness**\",\n",
    "    \"   - Provide a short, direct answer to the user question.\",\n",
    "    \"   - Do NOT list all video IDs, titles, or detailed reasoning in the final answer.\",\n",
    "    \"   - Only include sources internally for validation or logging if needed.\",    \n",
    "    \"\",\n",
    "    \"2. **Time awareness**\",\n",
    "    \"   - Take into account the time period being asked about.\",\n",
    "    \"   - Prefer newer and more accurate videos if conflicts exist.\",\n",
    "    \"\",\n",
    "    \"3. **Output**\",\n",
    "    \"   - final_answer: concise answer for the user\",\n",
    "    \"   - can_be_answered: true/false\",\n",
    "]\n",
    "JUDGE_PROMPT = \"\\n\".join(JUDGE_PROMPT)\n",
    "\n",
    "def main(*args, **kwargs):\n",
    "    available_collections = [d.name for d in qdrant.get_collections().collections]\n",
    "    print(\"Available collections in Qdrant:\")\n",
    "    for idx, name in enumerate(available_collections):\n",
    "        print(f\"{idx}: {name}\")\n",
    "    collection_index = int(input(\"Enter index of collection name to search: \"))\n",
    "    collection_name = available_collections[collection_index]\n",
    "    while True:\n",
    "        query = input(\"Enter your question: \")\n",
    "        videos = search_videos(collection_name, query)\n",
    "\n",
    "        for video in videos:\n",
    "            print(f\"Found video ID: {video.payload.get('video_id')} with score {video.score}, title: {video.payload.get('title')}\")\n",
    "        answers = []\n",
    "\n",
    "        for video in videos:\n",
    "            transcript = video.payload.get(\"transcript\", \"\")\n",
    "            video_id = video.payload.get(\"video_id\", \"unknown\")\n",
    "\n",
    "            if not transcript:\n",
    "                print(f\"‚ö†Ô∏è Skipping video {video_id}: no transcript available.\")\n",
    "                continue\n",
    "\n",
    "            # Use system prompt + structured response\n",
    "            structured_llm = llm.with_structured_output(QueryResponse)\n",
    "\n",
    "            response = structured_llm.invoke(\n",
    "                input=[\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": f\"Transcript:\\n{transcript}\\n\\nQuestion: {query}\"}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            if response.can_be_answered:\n",
    "                print(f\"\\n‚úÖ Can be answered in video {video_id}:\")\n",
    "                print(f\"Summary: {response.summary}\")\n",
    "                print(f\"Time range: {response.start_time} - {response.end_time}\")\n",
    "                answers.append({\n",
    "                    \"video_id\": video_id,\n",
    "                    \"title\": video.payload.get(\"title\"),\n",
    "                    \"upload_date\": video.payload.get(\"upload_date\"),\n",
    "                    \"score\": video.score,\n",
    "                    \"response\": response.model_dump()\n",
    "                })\n",
    "                # break\n",
    "            else:\n",
    "                print(f\"\\n‚ùå Cannot be answered in video {video_id}\")\n",
    "        judge_llm = llm.with_structured_output(FinalSummary)\n",
    "\n",
    "        judge_input = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"User question: {query}\\n\\nVideo responses:\\n\" +\n",
    "                    \"\\n\\n\".join([\n",
    "                        f\"Video: {a['title']} (ID: {a['video_id']}, Score: {a['score']:.3f})\\n\"\n",
    "                        f\"Video upload date: {a['upload_date']}\\n\"\n",
    "                        f\"Answer: {a['response']['summary']}\\n\"\n",
    "                        f\"Can be answered: {a['response']['can_be_answered']}\\n\"\n",
    "                        f\"Time range: {a['response']['start_time']} - {a['response']['end_time']}\"\n",
    "                        for a in answers\n",
    "                    ])\n",
    "        }\n",
    "\n",
    "        final_result = judge_llm.invoke([\n",
    "            {\"role\": \"system\", \"content\": JUDGE_PROMPT},\n",
    "            judge_input\n",
    "            ])\n",
    "\n",
    "        if final_result.can_be_answered:\n",
    "            print(\"\\n‚úÖ FINAL ANSWER:\")\n",
    "            print(f\"Summary: {final_result.final_answer}\")\n",
    "            print(f\"Sources: {', '.join(final_result.sources)}\")\n",
    "            print(f\"Time ranges: {', '.join(final_result.time_ranges)}\")\n",
    "        else:\n",
    "            print(\"\\n‚ùå Could not confidently answer the question from available videos.\")\n",
    "\n",
    "# === Example usage ===\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
